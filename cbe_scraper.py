# cbe_scraper.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©)
import os
import sys
import pandas as pd
from io import StringIO
import asyncio
from typing import Optional, Callable, Dict, Any
import logging
from contextlib import contextmanager
import time
from datetime import datetime, timedelta

from playwright.async_api import async_playwright, Page, Browser, BrowserContext
from bs4 import BeautifulSoup
import redis
from redis.exceptions import RedisError

from treasury_core.ports import YieldDataSource, HistoricalDataStore
import constants as C
from utils import log_performance_metrics, safe_divide, truncate_text

logger = logging.getLogger(__name__)


@contextmanager
def suppress_output():
    """Ù‚Ù…Ø¹ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª."""
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        sys.stdout = devnull
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr


class CbeScraper(YieldDataSource):
    """
    Ù…Ø­Ø³Ù† Ù„Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø°ÙˆÙ† Ø§Ù„Ø®Ø²Ø§Ù†Ø© Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ Ø§Ù„Ù…ØµØ±ÙŠ.
    
    Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
    - ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù…Ø¹ Redis
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡
    - ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø¹ Playwright
    - Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
    - ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø©."""
        self.redis_client = None
        self.cache_key = C.REDIS_CACHE_KEY
        self.cache_ttl_seconds = C.CACHE_TTL_SECONDS
        self.last_scrape_time = None
        self.scrape_stats = {
            "total_scrapes": 0,
            "successful_scrapes": 0,
            "failed_scrapes": 0,
            "cache_hits": 0,
            "cache_misses": 0,
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Redis Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
        self._setup_redis()
        
    def _setup_redis(self) -> None:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§ØªØµØ§Ù„ Redis Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡."""
        redis_uri = os.environ.get("AIVEN_REDIS_URI")
        if redis_uri:
            try:
                self.redis_client = redis.from_url(
                    redis_uri,
                    decode_responses=True,
                    socket_connect_timeout=5,
                    socket_timeout=5,
                    retry_on_timeout=True,
                    health_check_interval=30
                )
                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                self.redis_client.ping()
                logger.info("âœ… Redis client initialized successfully.")
            except RedisError as e:
                logger.error(f"âŒ Failed to initialize Redis client: {e}", exc_info=True)
                self.redis_client = None
        else:
            logger.warning("âš ï¸ AIVEN_REDIS_URI not set. Redis caching is disabled.")

    def _verify_page_structure(self, page_source: str) -> None:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµÙØ­Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª."""
        missing_markers = []
        
        for marker in C.ESSENTIAL_TEXT_MARKERS:
            if marker not in page_source:
                missing_markers.append(marker)
        
        if missing_markers:
            error_msg = f"Page structure verification failed! Missing markers: {missing_markers}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        logger.debug("âœ… Page structure verification passed")

    def _parse_cbe_html(self, page_source: str) -> Optional[pd.DataFrame]:
        """
        ØªØ­Ù„ÙŠÙ„ HTML Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡.
        """
        start_time = time.time()
        
        try:
            soup = BeautifulSoup(page_source, "lxml")
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            results_headers = soup.find_all(
                lambda tag: tag.name == "h2" and "Ø§Ù„Ù†ØªØ§Ø¦Ø¬" in tag.get_text()
            )
            
            if not results_headers:
                logger.warning("No results headers found in HTML")
                return None

            all_dataframes = []
            
            for header in results_headers:
                try:
                    section_df = self._parse_section(header)
                    if section_df is not None and not section_df.empty:
                        all_dataframes.append(section_df)
                except Exception as e:
                    logger.error(f"Error parsing section: {e}", exc_info=True)
                    continue

            if not all_dataframes:
                logger.warning("No valid data sections found")
                return None

            # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            final_df = pd.concat(all_dataframes, ignore_index=True)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
            final_df = self._process_dates(final_df)
            
            # ØªÙ†Ø¸ÙŠÙ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            final_df = self._clean_and_sort_data(final_df)
            
            # ØªØ³Ø¬ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
            log_performance_metrics(
                "_parse_cbe_html", 
                start_time, 
                {"sections_parsed": len(all_dataframes), "total_rows": len(final_df)}
            )
            
            return final_df
            
        except Exception as e:
            logger.error(f"âŒ Critical error during HTML parsing: {e}", exc_info=True)
            return None

    def _parse_section(self, header) -> Optional[pd.DataFrame]:
        """ØªØ­Ù„ÙŠÙ„ Ù‚Ø³Ù… ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
            dates_table = header.find_next("table")
            if not dates_table:
                return None

            # ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
            dates_df = pd.read_html(StringIO(str(dates_table)))[0]
            tenors = (
                pd.to_numeric(dates_df.columns[1:], errors="coerce")
                .dropna()
                .astype(int)
                .tolist()
            )
            
            if not tenors:
                return None

            session_dates_row = dates_df[dates_df.iloc[:, 0] == "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¬Ù„Ø³Ø©"]
            if session_dates_row.empty:
                return None

            session_dates = session_dates_row.iloc[0, 1 : len(tenors) + 1].tolist()
            
            # Ø¥Ù†Ø´Ø§Ø¡ DataFrame Ù„Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø¢Ø¬Ø§Ù„
            dates_tenors_df = pd.DataFrame({
                C.TENOR_COLUMN_NAME: tenors,
                C.SESSION_DATE_COLUMN_NAME: session_dates,
            })

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯
            accepted_bids_header = header.find_next(
                lambda tag: tag.name in ["p", "strong"]
                and C.ACCEPTED_BIDS_KEYWORD in tag.get_text()
            )
            
            if not accepted_bids_header:
                return None

            yields_table = accepted_bids_header.find_next("table")
            if not yields_table:
                return None

            # ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯
            yields_df_raw = pd.read_html(StringIO(str(yields_table)))[0]
            yields_df_raw.columns = ["Ø§Ù„Ø¨ÙŠØ§Ù†"] + tenors
            
            yield_row = yields_df_raw[
                yields_df_raw.iloc[:, 0].str.contains(C.YIELD_ANCHOR_TEXT, na=False)
            ]

            if yield_row.empty:
                return None

            yield_series = yield_row.iloc[0, 1:].astype(float)
            yield_series.name = C.YIELD_COLUMN_NAME
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            section_df = dates_tenors_df.join(yield_series, on=C.TENOR_COLUMN_NAME)

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if not section_df[C.YIELD_COLUMN_NAME].isnull().any():
                return section_df
                
        except Exception as e:
            logger.error(f"Error parsing section: {e}", exc_info=True)
            
        return None

    def _process_dates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª."""
        try:
            df["session_date_dt"] = pd.to_datetime(
                df[C.SESSION_DATE_COLUMN_NAME], 
                format=C.DATE_FORMAT, 
                errors="coerce"
            )
            
            df[C.DATE_COLUMN_NAME] = (
                df["session_date_dt"]
                .dt.tz_localize(
                    C.TIMEZONE, 
                    ambiguous="NaT", 
                    nonexistent="shift_forward"
                )
                .dt.tz_convert("UTC")
            )
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©
            df = df.dropna(subset=[C.DATE_COLUMN_NAME])
            
        except Exception as e:
            logger.error(f"Error processing dates: {e}", exc_info=True)
            
        return df

    def _clean_and_sort_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ØªÙ†Ø¸ÙŠÙ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
        try:
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¬Ù„Ø³Ø© (Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹)
            df = df.sort_values("session_date_dt", ascending=False)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£Ø­Ø¯Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ø£Ø¬Ù„
            df = df.drop_duplicates(subset=[C.TENOR_COLUMN_NAME])
            
            # ØªØ±ØªÙŠØ¨ Ù†Ù‡Ø§Ø¦ÙŠ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¬Ù„
            df = df.sort_values(by=C.TENOR_COLUMN_NAME)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø¤Ù‚Øª
            if "session_date_dt" in df.columns:
                df = df.drop(columns=["session_date_dt"])
                
        except Exception as e:
            logger.error(f"Error cleaning and sorting data: {e}", exc_info=True)
            
        return df

    async def _scrape_from_web_async(self) -> Optional[pd.DataFrame]:
        """
        Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Playwright Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø´Ø§Ù…Ù„Ø©.
        """
        start_time = time.time()
        browser = None
        context = None
        page = None
        
        try:
            logger.info("ðŸš€ Starting web scraping with Playwright...")
            
            with suppress_output():
                async with async_playwright() as p:
                    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØµÙØ­
                    browser = await p.chromium.launch(
                        headless=True,
                        args=[
                            "--disable-dev-shm-usage",
                            "--no-sandbox",
                            "--disable-setuid-sandbox",
                            "--disable-gpu",
                            "--disable-web-security",
                            "--disable-features=VizDisplayCompositor"
                        ]
                    )
                    
                    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚
                    context = await browser.new_context(
                        user_agent=C.USER_AGENT,
                        viewport={"width": 1920, "height": 1080},
                        extra_http_headers={
                            "Accept-Language": "ar-EG,ar;q=0.9,en;q=0.8",
                            "Accept-Encoding": "gzip, deflate, br",
                            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                        }
                    )
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙØ­Ø©
                    page = await context.new_page()
                    
                    # ØªØ¹ÙŠÙŠÙ† Ù…Ù‡Ù„Ø© Ø£Ø·ÙˆÙ„ Ù„Ù„ØªØ­Ù…ÙŠÙ„
                    page.set_default_timeout(C.SCRAPER_TIMEOUT_SECONDS * 1000)
                    
                    # Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©
                    logger.debug(f"Navigating to: {C.CBE_DATA_URL}")
                    await page.goto(C.CBE_DATA_URL, wait_until="networkidle")
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
                    await page.wait_for_selector(
                        "table", 
                        timeout=C.SCRAPER_TIMEOUT_SECONDS * 1000
                    )
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± Ø¥Ø¶Ø§ÙÙŠ Ù„Ø¶Ù…Ø§Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                    await page.wait_for_timeout(2000)
                    
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
                    page_source = await page.content()
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„
                    self._verify_page_structure(page_source)
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    result = self._parse_cbe_html(page_source)
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    self.scrape_stats["total_scrapes"] += 1
                    if result is not None and not result.empty:
                        self.scrape_stats["successful_scrapes"] += 1
                        self.last_scrape_time = datetime.now()
                    else:
                        self.scrape_stats["failed_scrapes"] += 1
                    
                    # ØªØ³Ø¬ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
                    log_performance_metrics(
                        "_scrape_from_web_async", 
                        start_time, 
                        {"rows_scraped": len(result) if result is not None else 0}
                    )
                    
                    return result
                    
        except Exception as e:
            logger.error(f"âŒ Playwright scraping failed: {e}", exc_info=True)
            self.scrape_stats["failed_scrapes"] += 1
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªÙ‚Ø§Ø· Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù„Ù„ØªØ´Ø®ÙŠØµ
            if page:
                try:
                    screenshot_path = "/app/debug_screenshot.png"
                    await page.screenshot(
                        path=screenshot_path, 
                        full_page=True
                    )
                    logger.warning(f"ðŸ“¸ Screenshot saved at {screenshot_path}")
                except Exception as ss_err:
                    logger.warning(f"âš ï¸ Failed to capture screenshot: {ss_err}")
            
            return None
            
        finally:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            if context:
                try:
                    await context.close()
                except Exception:
                    pass
            if browser:
                try:
                    await browser.close()
                except Exception:
                    pass

    async def get_latest_yields_async(
        self, force_refresh: bool = False
    ) -> Optional[pd.DataFrame]:
        """
        Ø¬Ù„Ø¨ Ø£Ø­Ø¯Ø« Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª.
        
        Args:
            force_refresh: Ø¥Ø°Ø§ ÙƒØ§Ù† TrueØŒ ÙŠØªØ¬Ø§ÙˆØ² Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ None
        """
        start_time = time.time()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if not force_refresh and self.redis_client:
            cached_data = await self._get_cached_data()
            if cached_data is not None:
                self.scrape_stats["cache_hits"] += 1
                logger.info("âœ… Cache hit! Loading data from Redis.")
                return cached_data
            else:
                self.scrape_stats["cache_misses"] += 1

        if force_refresh:
            logger.info("ðŸ”„ Force refresh enabled, bypassing cache.")

        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ÙˆÙŠØ¨
        live_data = await self._scrape_from_web_async()

        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if self.redis_client and live_data is not None and not live_data.empty:
            await self._cache_data(live_data)

        # ØªØ³Ø¬ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
        log_performance_metrics(
            "get_latest_yields_async", 
            start_time, 
            {"force_refresh": force_refresh, "cache_used": not force_refresh}
        )
        
        return live_data

    async def _get_cached_data(self) -> Optional[pd.DataFrame]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª."""
        try:
            cached_data = self.redis_client.get(self.cache_key)
            if cached_data:
                df = pd.read_json(StringIO(cached_data), lines=True)
                df[C.DATE_COLUMN_NAME] = pd.to_datetime(
                    df[C.DATE_COLUMN_NAME], 
                    errors="coerce", 
                    utc=True
                )
                return df
        except RedisError as e:
            logger.error(f"Redis cache read error: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Error parsing cached data: {e}", exc_info=True)
        
        return None

    async def _cache_data(self, df: pd.DataFrame) -> None:
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª."""
        try:
            logger.info(f"ðŸ’¾ Storing new data in Redis cache for {self.cache_ttl_seconds} seconds.")
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ JSON
            json_data = df.to_json(orient="records", lines=True, date_format="iso")
            
            # Ø­ÙØ¸ ÙÙŠ Redis
            self.redis_client.setex(
                self.cache_key,
                self.cache_ttl_seconds,
                json_data
            )
            
            logger.info("âœ… Data cached successfully")
            
        except RedisError as e:
            logger.error(f"Redis cache write error: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Error caching data: {e}", exc_info=True)

    def get_latest_yields(self) -> Optional[pd.DataFrame]:
        """ÙˆØ§Ø¬Ù‡Ø© Ù…ØªØ²Ø§Ù…Ù†Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
        return asyncio.run(self.get_latest_yields_async())

    def get_scrape_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø¨."""
        return {
            **self.scrape_stats,
            "last_scrape_time": self.last_scrape_time.isoformat() if self.last_scrape_time else None,
            "redis_available": self.redis_client is not None,
            "cache_ttl_seconds": self.cache_ttl_seconds
        }

    def clear_cache(self) -> bool:
        """Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª."""
        try:
            if self.redis_client:
                self.redis_client.delete(self.cache_key)
                logger.info("âœ… Cache cleared successfully")
                return True
        except RedisError as e:
            logger.error(f"Error clearing cache: {e}", exc_info=True)
        return False


async def fetch_and_update_data_async(
    data_source: CbeScraper,
    data_store: HistoricalDataStore,
    status_callback: Optional[Callable[[str], None]] = None,
    force_refresh: bool = False,
) -> bool:
    """
    Ø¬Ù„Ø¨ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªØ²Ø§Ù…Ù† Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø´Ø§Ù…Ù„Ø©.
    
    Args:
        data_source: Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_store: Ù…Ø®Ø²Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        status_callback: Ø¯Ø§Ù„Ø© Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
        force_refresh: Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„ØªØ­Ø¯ÙŠØ«
        
    Returns:
        True Ø¥Ø°Ø§ ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ù†Ø¬Ø§Ø­ØŒ False Ø®Ù„Ø§Ù Ø°Ù„Ùƒ
    """
    start_time = time.time()
    
    try:
        if status_callback:
            status_callback("Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

        # Ø¬Ù„Ø¨ Ø£Ø­Ø¯Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        latest_data = await data_source.get_latest_yields_async(force_refresh=force_refresh)

        if latest_data is not None and not latest_data.empty:
            if status_callback:
                status_callback("ØªÙ… Ø§Ù„Ø¬Ù„Ø¨ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù‚Ù‚...")
                
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
            db_session_date_str = data_store.get_latest_session_date()
            live_latest_date_str = latest_data[C.SESSION_DATE_COLUMN_NAME].iloc[0]

            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ù…Ø·Ù„ÙˆØ¨ ØªØ­Ø¯ÙŠØ« Ø¥Ø¬Ø¨Ø§Ø±ÙŠ
            if (
                force_refresh
                or not db_session_date_str
                or live_latest_date_str != db_session_date_str
            ):
                if status_callback:
                    status_callback("ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø­ÙØ¸...")
                    
                data_store.save_data(latest_data)
                
                if status_callback:
                    status_callback("Ø§ÙƒØªÙ…Ù„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
                    
                # ØªØ³Ø¬ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
                log_performance_metrics(
                    "fetch_and_update_data_async", 
                    start_time, 
                    {"rows_updated": len(latest_data), "force_refresh": force_refresh}
                )
                
                return True
            else:
                if status_callback:
                    status_callback("Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯Ø«Ø© Ø¨Ø§Ù„ÙØ¹Ù„.")
                return False
        else:
            raise RuntimeError("ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±")

    except Exception as e:
        logger.error(f"Error in fetch_and_update_data_async: {e}", exc_info=True)
        if status_callback:
            status_callback("ÙØ´Ù„ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        raise


def fetch_and_update_data(
    data_source: YieldDataSource,
    data_store: HistoricalDataStore,
    status_callback: Optional[Callable[[str], None]] = None,
) -> bool:
    """ÙˆØ§Ø¬Ù‡Ø© Ù…ØªØ²Ø§Ù…Ù†Ø© Ù„Ø¬Ù„Ø¨ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    return asyncio.run(
        fetch_and_update_data_async(data_source, data_store, status_callback)
    )
